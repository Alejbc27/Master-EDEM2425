# From Foundations to Cloud: A Data Engineering Journey ğŸ’¡

**Welcome! ğŸ‘‹**  
This repository is a technical log of my journey through the **Masterâ€™s in Big Data & Cloud at EDEM (Valencia, Spain).**  
Itâ€™s more than just code â€” it showcases how I think, build, and solve real-world data and cloud problems.

The structure follows a **technical journal format**: each section represents a learning milestone, paired with hands-on implementation, automation practices, and design decisions that reflect how I approach engineering challenges.

---

## ğŸ¯ Purpose

> Demonstrate how theoretical knowledge translates into real, scalable, and reproducible solutions.

---

## ğŸ› ï¸ Tech Stack

**Languages & Tools**  
`Python` Â· `SQL` Â· `Docker` Â· `Git` Â· `GitHub Actions` Â· `Airflow` Â· `Terraform` Â· `DBT`

**Cloud & Infrastructure**  
`AWS` Â· `GCP` Â· `Azure` Â· `Snowflake` Â· `Cloud Run` Â· `Cloud Functions` Â· `Pub/Sub` Â· `Dataflow`

**Data Processing & Engineering**  
`ETL` Â· `Data Warehousing` Â· `Kafka` Â· `Spark` Â· `NiFi` Â· `CDC` Â· `Kubernetes` Â· `Microservices` 

**Visualization & BI**  
`Power BI` Â· `Tableau` Â· `Metabase` Â· `Grafana`

**Other**  
`Machine Learning` Â· `NoSQL`

---

## ğŸ§  Module 1 â€“ Foundations of Data & Development  
ğŸ“ [`/MODULE-1`](MODULE-1)

- Explored data types, sources, governance, and ethical use  
- Worked with Linux and Git to manage environments and version control  
- Wrote Python scripts for automation and basic data processing  
- Queried relational databases using SQL  
- Created and managed containers with Docker  
- Learned how projects are structured and delivered using modern development workflows  

ğŸ” **Goal:** Build the technical fundamentals required to handle more advanced data engineering challenges

---

## ğŸ”„ Module 2 â€“ Data Processing & Engineering  
ğŸ“ [`/MODULE-2`](MODULE-2)

- Designed ETL pipelines using tools like NiFi and DBT  
- Applied data transformation logic and modeling techniques  
- Processed data in batch and real-time using PySpark and Kafka  
- Worked with NoSQL systems and implemented Change Data Capture (CDC)  
- Ensured data quality and governance across the pipeline  
- Built dashboards and visualizations to deliver actionable insights  

ğŸ” **Goal:** Master the lifecycle of a data pipeline â€” from ingestion to processing and storage

---

## â˜ï¸ Module 3 â€“ Cloud & Automation: Built to Scale  
ğŸ“ [`/MODULE-3`](MODULE-3)

- Deployed data architectures on AWS, GCP, and Azure  
- Used Terraform for Infrastructure as Code across all platforms  
- Built CI/CD workflows with GitHub Actions for automation  
- Orchestrated data pipelines using Apache Airflow  
- Managed cloud services for compute, storage, and data flow  
- Explored Snowflake as a scalable cloud data warehouse  

ğŸ” **Goal:** Apply DevOps principles to deliver robust cloud-based systems

---

## ğŸ’¼ Real-World Projects  
ğŸ“ [`/DATA-PROJECTS`](DATA-PROJECTS)

### ğŸ›°ï¸ Data Project 1 â€“ FindMyDistrict *(Dec 2024)*  
**District Preference Analysis in Valencia using Hierarchical Models**

- Developed a Python app using Valencia and OpenStreetMaps data to analyze district preferences  
- Collected user input on factors like price, services, and transport for multicriteria decision analysis  
- Stored data in PostgreSQL with Docker and applied AHP matrices for informed recommendations  
- Built an interactive Streamlit dashboard with business metrics to support decision-makers  

---

### ğŸŒ Data Project 2 â€“ Dana-Xarxa *(Feb 2025)*  
**Emergency Coordination Platform on GCP**

- Built a GCP-based app to coordinate emergency services and volunteers  
- Deployed infrastructure with Terraform, using Cloud Run, Pub/Sub, and Dataflow for matching logic  
- Managed Docker images with Artifact Registry and stored analytics in BigQuery  
- Implemented real-time notifications via Cloud Functions and visualized data in Grafana  

---

### ğŸ§  Data Project 3 â€“ MediQuestAI *(May 2025)*  
**Infrastructure and APIs for an AI-powered Medical Assistant**

- Developed backend APIs (appointments, email, status, RAG) to enhance an AI-based assistant  
- Automated clinical workflows with Cloud Run, Firestore, PostgreSQL, and BigQuery  
- Provisioned reproducible infrastructure using Terraform on GCP to optimize cost and scalability  
- Designed ETL pipelines and Metabase dashboards for clinical data analysis  
- Integrated services with the AI agent to ensure scalability and maintainability  

ğŸ§© **Focus:** Event-driven architecture with real-world impact and cloud scalability

---

## ğŸ§­ Key Takeaways

âœ”ï¸ Automation saves time now â€” and headaches later  
âœ”ï¸ Repetition builds clarity â€” every iteration improves the architecture  
âœ”ï¸ Learning by doing builds real engineering intuition  

---

## ğŸ¤ Letâ€™s Connect

I'm open to new opportunities, ideas, and technical challenges.  
ğŸ“« Reach out via [LinkedIn](https://www.linkedin.com/in/alejandroboschcervera/) or explore more on my GitHub profile.
