# From Foundations to Cloud: A Data Engineering Journey 💡

**Welcome! 👋**  
This repository is a technical log of my journey through the **Master’s in Big Data & Cloud at EDEM (Valencia, Spain).**  
It’s more than just code — it showcases how I think, build, and solve real-world data and cloud problems.

The structure follows a **technical journal format**: each section represents a learning milestone, paired with hands-on implementation, automation practices, and design decisions that reflect how I approach engineering challenges.

---

## 🎯 Purpose

> Demonstrate how theoretical knowledge translates into real, scalable, and reproducible solutions.

---

## 🛠️ Tech Stack

**Languages & Tools**  
`Python` · `SQL` · `Docker` · `Git` · `GitHub Actions` · `Airflow` · `Terraform` · `DBT`

**Cloud & Infrastructure**  
`AWS` · `GCP` · `Azure` · `Snowflake` · `Cloud Run` · `Cloud Functions` · `Pub/Sub` · `Dataflow`

**Data Processing & Engineering**  
`ETL` · `Data Warehousing` · `Kafka` · `Spark` · `NiFi` · `CDC` · `Kubernetes` · `Microservices` 

**Visualization & BI**  
`Power BI` · `Tableau` · `Metabase` · `Grafana`

**Other**  
`Machine Learning` · `NoSQL`

---

## 🧠 Module 1 – Foundations of Data & Development  
📁 [`/MODULE-1`](MODULE-1)

- Explored data types, sources, governance, and ethical use  
- Worked with Linux and Git to manage environments and version control  
- Wrote Python scripts for automation and basic data processing  
- Queried relational databases using SQL  
- Created and managed containers with Docker  
- Learned how projects are structured and delivered using modern development workflows  

🔍 **Goal:** Build the technical fundamentals required to handle more advanced data engineering challenges

---

## 🔄 Module 2 – Data Processing & Engineering  
📁 [`/MODULE-2`](MODULE-2)

- Designed ETL pipelines using tools like NiFi and DBT  
- Applied data transformation logic and modeling techniques  
- Processed data in batch and real-time using PySpark and Kafka  
- Worked with NoSQL systems and implemented Change Data Capture (CDC)  
- Ensured data quality and governance across the pipeline  
- Built dashboards and visualizations to deliver actionable insights  

🔍 **Goal:** Master the lifecycle of a data pipeline — from ingestion to processing and storage

---

## ☁️ Module 3 – Cloud & Automation: Built to Scale  
📁 [`/MODULE-3`](MODULE-3)

- Deployed data architectures on AWS, GCP, and Azure  
- Used Terraform for Infrastructure as Code across all platforms  
- Built CI/CD workflows with GitHub Actions for automation  
- Orchestrated data pipelines using Apache Airflow  
- Managed cloud services for compute, storage, and data flow  
- Explored Snowflake as a scalable cloud data warehouse  

🔍 **Goal:** Apply DevOps principles to deliver robust cloud-based systems

---

## 💼 Real-World Projects  
📁 [`/DATA-PROJECTS`](DATA-PROJECTS)

### 🛰️ Data Project 1 – FindMyDistrict *(Dec 2024)*  
**District Preference Analysis in Valencia using Hierarchical Models**

- Developed a Python app using Valencia and OpenStreetMaps data to analyze district preferences  
- Collected user input on factors like price, services, and transport for multicriteria decision analysis  
- Stored data in PostgreSQL with Docker and applied AHP matrices for informed recommendations  
- Built an interactive Streamlit dashboard with business metrics to support decision-makers  

---

### 🌐 Data Project 2 – Dana-Xarxa *(Feb 2025)*  
**Emergency Coordination Platform on GCP**

- Built a GCP-based app to coordinate emergency services and volunteers  
- Deployed infrastructure with Terraform, using Cloud Run, Pub/Sub, and Dataflow for matching logic  
- Managed Docker images with Artifact Registry and stored analytics in BigQuery  
- Implemented real-time notifications via Cloud Functions and visualized data in Grafana  

---

### 🧠 Data Project 3 – MediQuestAI *(May 2025)*  
**Infrastructure and APIs for an AI-powered Medical Assistant**

- Developed backend APIs (appointments, email, status, RAG) to enhance an AI-based assistant  
- Automated clinical workflows with Cloud Run, Firestore, PostgreSQL, and BigQuery  
- Provisioned reproducible infrastructure using Terraform on GCP to optimize cost and scalability  
- Designed ETL pipelines and Metabase dashboards for clinical data analysis  
- Integrated services with the AI agent to ensure scalability and maintainability  

🧩 **Focus:** Event-driven architecture with real-world impact and cloud scalability

---

## 🧭 Key Takeaways

✔️ Automation saves time now — and headaches later  
✔️ Repetition builds clarity — every iteration improves the architecture  
✔️ Learning by doing builds real engineering intuition  

---

## 🤝 Let’s Connect

I'm open to new opportunities, ideas, and technical challenges.  
📫 Reach out via [LinkedIn](https://www.linkedin.com/in/alejandroboschcervera/) or explore more on my GitHub profile.
