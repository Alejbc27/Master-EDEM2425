# ⚙️ MODULE 2: Data Processing & Engineering

This module focused on the development of modern, scalable data pipelines — both batch and streaming — to transform raw data into structured, reliable, and usable assets.

Throughout the process, I worked with key data engineering tools and patterns to manage ingestion, transformation, validation, and delivery across various systems.

---

## ✅ Key Learnings

🔄 **Data Ingestion with Apache NiFi**  
Designed flow-based pipelines to automate data movement between systems and apply real-time processing logic.

📡 **Change Data Capture (CDC)**  
Implemented patterns to capture and stream changes from source databases, enabling near real-time synchronization.

🍃 **NoSQL with MongoDB**  
Explored document-oriented databases to store and query semi-structured data efficiently.

📬 **Streaming with Apache Kafka**  
Developed real-time pipelines using Kafka producers and consumers, including topic management and message filtering.

⚡ **Distributed Processing with PySpark**  
Applied PySpark for scalable data transformations, aggregations, and analytics on large datasets.

🛠️ **Data Modeling with DBT**  
Created modular, version-controlled transformation models directly within the data warehouse.

📊 **Visualization with Tableau & Power BI**  
Designed dashboards to communicate insights and support decision-making using modern BI tools.

---

## 📂 Folder Overview

📬 **Kafka Streaming Project**  
A real-time data pipeline using Kafka: JSON messages are produced, consumed, filtered, and streamed to new topics. Includes real-time querying via KSQL.

⚡ **PySpark Data Project**  
End-to-end batch pipeline including data cleaning, joins, and aggregations across multiple datasets. Final output is loaded into a MySQL database.

🛠️ **DBT Transformations**  
SQL-based models for transforming raw data into structured outputs inside the data warehouse, fully integrated with version control.

📊 **BI Dashboards**  
Dashboard examples built in Tableau and Power BI to visualize processed data and extract key business insights.

---

🎯 *Goal of the Module:*  
To build reliable, automated data pipelines that deliver clean, trusted data ready for analytics, reporting, or production use — at scale and in real time.
